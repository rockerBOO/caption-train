# Caption-Train Documentation

## Advanced Techniques

- [Caption Enhancement](advanced/caption_improvement.md)
  - Techniques for improving image captions using VLM and LLM

- [Data Utilities](advanced/data_utils.md)
  - Tools for dataset management, image processing, and analysis

- [Image Viewer](advanced/image_viewer.md)
  - Lightweight server for browsing image datasets

- [Servers](advanced/servers.md)
  - Captioning and inference server configurations

- [Training Techniques](advanced/training_techniques.md)
  - Advanced model training strategies

## Model Documentation

- [BLIP Model](models/blip.md)
  - Training and inference for BLIP models

- [Florence Model](models/florence.md)
  - Detailed guide for Florence-2 model usage

- [GIT Model](models/git.md)
  - Generative Image-to-Text model documentation

- [Moondream Model](models/moondream.md)
  - Compact vision-language model guide

- [Qwen Model](models/qwen.md)
  - Qwen 2.5 Vision-Language model details

## Contributing

- Help improve our documentation by submitting pull requests
- Report any inaccuracies or missing information
- Provide examples and use cases
