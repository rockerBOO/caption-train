# enhance_captions.py Documentation

## Overview

`enhance_captions.py` is a script designed to improve image captions by combining manual captions with those generated by Vision Language Models (VLMs) and Large Language Models (LLMs). The script implements a three-stage pipeline:

1. Load existing manual captions for images
2. Generate captions using a Vision Language Model
3. Combine both captions using a Large Language Model to create enhanced, detailed descriptions

## Features

### Image Loading & Processing

- Supports multiple image formats: PNG, JPG, JPEG, WebP
- Recursively searches directories for images
- Pairs images with their corresponding caption files

### Caption Generation

- Uses Vision Language Models (VLMs) like Microsoft Florence or Janus for automated caption generation
- Supports custom prompts to guide the VLM caption generation process
- Compatible with HuggingFace's model ecosystem (AutoModelForCausalLM, AutoProcessor)

### Caption Enhancement

- Combines manually created captions with VLM-generated captions
- Uses LLMs to create richer, more detailed final captions
- Supports OpenAI-compatible API endpoints (including local LLM servers)

### Performance & Optimization

- Uses Accelerate library for efficient model loading and inference
- Implements intelligent caching to avoid redundant processing
- Includes rate limiting for API calls to prevent throttling
- Automatically manages model placement between CPU/GPU as needed

### Caching System

- Caches VLM-generated captions as `{image_name}_generated.txt`
- Caches final combined captions as `{image_name}_combined.txt`
- Resumes processing from cached files if the script is interrupted and restarted

## Usage

```bash
accelerate launch enhance_captions.py \
  --system_prompt system_prompt.txt \
  --prompt "<MORE_DETAILED_CAPTION>" \
  --dataset_dir images/ \
  --model_id microsoft/Florence-2-large-ft \
  --trust_remote_code \
  --base_url http://127.0.0.1:5000/v1/
```

## Command-Line Arguments

| Argument                             | Description                                              |
| ------------------------------------ | -------------------------------------------------------- |
| `--dataset_dir`                      | Directory containing images and caption files (required) |
| `--prompt`                           | Prompt text for the VLM caption generation               |
| `--system_prompt`                    | Path to file containing system prompt for the LLM        |
| `--model_id`                         | HuggingFace model ID for the VLM (required)              |
| `--revision`                         | Revision/branch of the HuggingFace model (default: main) |
| `--trust_remote_code`                | Flag to trust remote code from HuggingFace               |
| `--base_url`                         | Base URL for the LLM API (e.g., local server)            |
| `--model`                            | LLM model to use (default: llama-3.3-70b-versatile)      |
| `--cache_generated_captions_to_disk` | Flag to enable disk caching of generated captions        |

## Processing Flow

1. Load images and their manual captions (`{name}.txt`)
2. Generate captions using the specified VLM
3. Cache VLM-generated captions (`{name}_generated.txt`)
4. Combine manual and VLM captions using an LLM
5. Cache the final combined captions (`{name}_combined.txt`)

## Requirements

- PyTorch
- OpenAI or compatible API
- Transformers library
- Accelerate library
- PIL (Python Imaging Library)
- tqdm (for progress bars)

## Notes

- The script requires an OpenAI API key set in environment variables or a `.env` file
- Local LLM servers with OpenAI-compatible APIs are supported and recommended
- Models with `trust_remote_code` requirements (like Janus) are supported
